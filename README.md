# FederatedLearning
CS-E4740 - Federated Learning 

##Abstract

Many application domains of machine learning (ML), such as numerical weather prediction, generate decentralized 
collections of local datasets. A naive application of basic ML methods [1,2] would require collecting these local datasets 
at a central point. However, this approach might be unfavorable for several reasons, including inefficient use of 
computational infrastructure and lack of privacy protection.

Federated learning (FL) aims at training ML models in a decentralized and collaborative fashion. FL methods require only the 
exchange of model parameter updates instead of raw data. These methods are appealing computationally and from a 
privacy protection perspective. Indeed, FL methods leverage distributed computational resources and minimize the leakage 
of private information irrelevant to the learning task.

This course uses an optimization perspective to study some widely used FL models and algorithms. In particular, we will 
discuss total variation minimization as a unifying design principle for FL methods. The geometry of total variation 
determines these methods' computational and statistical properties. 


## Background Material 

- [Federated learning: Basics and application to the mobile keyboard (ML Tech Talks)](https://www.youtube.com/watch?v=IXI1AjimfmE)



